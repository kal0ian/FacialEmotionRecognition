{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import ConvLSTM2D, Activation\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "    model_json = model.to_json()\n",
    "    with open(filename + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(filename + \".h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"./data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_parquet(DATA_FOLDER + \"training.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_test = pd.read_parquet(DATA_FOLDER + \"public_test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_private_test = pd.read_parquet(DATA_FOLDER + \"private_test.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\python3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "x_train = df_training.as_matrix()[:,1:]\n",
    "y_train = df_training.as_matrix()[:,0]\n",
    "x_test = df_public_test.as_matrix()[:,1:]\n",
    "y_test = df_public_test.as_matrix()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 48, 48, 1)\n",
    "x_test = x_test.reshape(-1, 48, 48, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(\"float32\")/255.\n",
    "x_test = x_test.astype(\"float32\")/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, 3, data_format=\"channels_last\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape=(48, 48, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(64, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Conv2D(32, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(32, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "\n",
    "model.add(Conv2D(32, 3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.6))\n",
    "\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range = 0.1,\n",
    "                            height_shift_range = 0.1,\n",
    "                            width_shift_range = 0.1,\n",
    "                            rotation_range = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-4), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 675s 1s/step - loss: 2.1542 - acc: 0.2044 - val_loss: 1.7786 - val_acc: 0.3175\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 684s 1s/step - loss: 1.8688 - acc: 0.2461 - val_loss: 1.7286 - val_acc: 0.3225\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 752s 2s/step - loss: 1.7953 - acc: 0.2700 - val_loss: 1.7102 - val_acc: 0.2950\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 794s 2s/step - loss: 1.7686 - acc: 0.2780 - val_loss: 1.6968 - val_acc: 0.3425\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 703s 1s/step - loss: 1.7384 - acc: 0.2984 - val_loss: 1.6621 - val_acc: 0.3225\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 683s 1s/step - loss: 1.7008 - acc: 0.3138 - val_loss: 1.6361 - val_acc: 0.3675\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 691s 1s/step - loss: 1.6955 - acc: 0.3181 - val_loss: 1.5994 - val_acc: 0.3700\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 628s 1s/step - loss: 1.6765 - acc: 0.3302 - val_loss: 1.5188 - val_acc: 0.4125\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 631s 1s/step - loss: 1.6496 - acc: 0.3411 - val_loss: 1.4699 - val_acc: 0.4400\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 629s 1s/step - loss: 1.6201 - acc: 0.3665 - val_loss: 1.4461 - val_acc: 0.4375\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 632s 1s/step - loss: 1.6012 - acc: 0.3735 - val_loss: 1.5043 - val_acc: 0.4275\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 631s 1s/step - loss: 1.5818 - acc: 0.3866 - val_loss: 1.4102 - val_acc: 0.4600\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 633s 1s/step - loss: 1.5798 - acc: 0.3845 - val_loss: 1.3955 - val_acc: 0.4550\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 682s 1s/step - loss: 1.5416 - acc: 0.4052 - val_loss: 1.3927 - val_acc: 0.5025\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 705s 1s/step - loss: 1.5286 - acc: 0.4051 - val_loss: 1.3685 - val_acc: 0.4700\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 773s 2s/step - loss: 1.5445 - acc: 0.4034 - val_loss: 1.3194 - val_acc: 0.5100\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 910s 2s/step - loss: 1.5194 - acc: 0.4085 - val_loss: 1.3583 - val_acc: 0.4825\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 839s 2s/step - loss: 1.5033 - acc: 0.4233 - val_loss: 1.3174 - val_acc: 0.5100\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 705s 1s/step - loss: 1.5029 - acc: 0.4175 - val_loss: 1.3432 - val_acc: 0.4925\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 656s 1s/step - loss: 1.4923 - acc: 0.4296 - val_loss: 1.2968 - val_acc: 0.5175\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16),\n",
    "                           steps_per_epoch=500,\n",
    "                           epochs=20, #Increase this when not on Kaggle kernel\n",
    "                           validation_data=(x_test[:400,:], y_test[:400,:]), #For speed\n",
    "                           callbacks=[annealer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40\n",
      "500/500 [==============================] - 516s 1s/step - loss: 1.3772 - acc: 0.4612 - val_loss: 1.2512 - val_acc: 0.5450\n",
      "Epoch 22/40\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.3944 - acc: 0.4564 - val_loss: 1.2297 - val_acc: 0.5475\n",
      "Epoch 23/40\n",
      "500/500 [==============================] - 521s 1s/step - loss: 1.3660 - acc: 0.4641 - val_loss: 1.2258 - val_acc: 0.5600\n",
      "Epoch 24/40\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.3788 - acc: 0.4586 - val_loss: 1.2210 - val_acc: 0.5400\n",
      "Epoch 25/40\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.3569 - acc: 0.4647 - val_loss: 1.2043 - val_acc: 0.5500\n",
      "Epoch 26/40\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.3376 - acc: 0.4768 - val_loss: 1.1927 - val_acc: 0.5500\n",
      "Epoch 27/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.3527 - acc: 0.4777 - val_loss: 1.1985 - val_acc: 0.5525\n",
      "Epoch 28/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.3451 - acc: 0.4763 - val_loss: 1.1938 - val_acc: 0.5525\n",
      "Epoch 29/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.3470 - acc: 0.4751 - val_loss: 1.1806 - val_acc: 0.5675\n",
      "Epoch 30/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.3217 - acc: 0.4913 - val_loss: 1.1755 - val_acc: 0.5750\n",
      "Epoch 31/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.3011 - acc: 0.4933 - val_loss: 1.1689 - val_acc: 0.5725\n",
      "Epoch 32/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.3199 - acc: 0.4818 - val_loss: 1.1643 - val_acc: 0.5750\n",
      "Epoch 33/40\n",
      "500/500 [==============================] - 518s 1s/step - loss: 1.3092 - acc: 0.4975 - val_loss: 1.1581 - val_acc: 0.5750\n",
      "Epoch 34/40\n",
      "500/500 [==============================] - 518s 1s/step - loss: 1.3100 - acc: 0.4931 - val_loss: 1.1604 - val_acc: 0.5800\n",
      "Epoch 35/40\n",
      "500/500 [==============================] - 518s 1s/step - loss: 1.3185 - acc: 0.4879 - val_loss: 1.1550 - val_acc: 0.5775\n",
      "Epoch 36/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.2964 - acc: 0.4971 - val_loss: 1.1609 - val_acc: 0.5775\n",
      "Epoch 37/40\n",
      "500/500 [==============================] - 519s 1s/step - loss: 1.3067 - acc: 0.4934 - val_loss: 1.1560 - val_acc: 0.5875\n",
      "Epoch 38/40\n",
      "500/500 [==============================] - 517s 1s/step - loss: 1.2998 - acc: 0.4890 - val_loss: 1.1521 - val_acc: 0.5775\n",
      "Epoch 39/40\n",
      "500/500 [==============================] - 520s 1s/step - loss: 1.2873 - acc: 0.5050 - val_loss: 1.1445 - val_acc: 0.5800\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 517s 1s/step - loss: 1.3023 - acc: 0.4958 - val_loss: 1.1462 - val_acc: 0.5850\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16),\n",
    "                           steps_per_epoch=500,\n",
    "                           initial_epoch=20,\n",
    "                           epochs=40, #Increase this when not on Kaggle kernel\n",
    "                           validation_data=(x_test[:400,:], y_test[:400,:]), #For speed\n",
    "                           callbacks=[annealer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss, final_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "save_model(model, \"./trained_models/fifth_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
